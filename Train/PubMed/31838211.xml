<?xml version='1.0' encoding='utf-8'?>
<document id="31838211"><sentence text="Testing the face validity and inter-rater agreement of a simple approach to drug-drug interaction evidence assessment." /><sentence text="Low concordance between drug-drug interaction (DDI) knowledge bases is a well-documented concern" /><sentence text=" One potential cause of inconsistency is variability between drug experts in approach to assessing evidence about potential DDIs" /><sentence text=" In this study, we examined the face validity and inter-rater reliability of a novel DDI evidence evaluation instrument designed to be simple and easy to use" /><sentence text="" /><sentence text="A convenience sample of participants with professional experience evaluating DDI evidence was recruited" /><sentence text=" Participants independently evaluated pre-selected evidence items for 5 drug pairs using the new instrument" /><sentence text=" For each drug pair, participants labeled each evidence item as sufficient or insufficient to establish the existence of a DDI based on the evidence categories provided by the instrument" /><sentence text=" Participants also decided if the overall body of evidence supported a DDI involving the drug pair" /><sentence text=" Agreement was computed both at the evidence item and drug pair levels" /><sentence text=" A cut-off of ≥ 70% was chosen as the agreement threshold for percent agreement, while a coefficient &gt; 0" /><sentence text="6 was used as the cut-off for chance-corrected agreement" /><sentence text=" Open ended comments were collected and coded to identify themes related to the participants' experience using the novel approach" /><sentence text="" /><sentence text="The face validity of the new instrument was established by two rounds of evaluation involving a total of 6 experts" /><sentence text=" Fifteen experts agreed to participate in the reliability assessment, and 14 completed the study" /><sentence text=" Participant agreement on the sufficiency of 22 of the 34 evidence items (65%) did not exceed the a priori agreement threshold" /><sentence text=" Similarly, agreement on the sufficiency of evidence for 3 of the 5 drug pairs (60%) was poor" /><sentence text=" Chance-corrected agreement at the drug pair level further confirmed the poor interrater reliability of the instrument (Gwet's AC1 = 0" /><sentence text="24, Conger's Kappa = 0" /><sentence text="24)" /><sentence text=" Participant comments suggested several possible reasons for the disagreements including unaddressed subjectivity in assessing an evidence item's type and study design, an infeasible separation of evidence evaluation from the consideration of clinical relevance, and potential issues related to the evaluation of DDI case reports" /><sentence text="" /><sentence text="Even though the key findings were negative, the study's results shed light on how experts approach DDI evidence assessment, including the importance situating evidence assessment within the context of consideration of clinical relevance" /><sentence text=" Analysis of participant comments within the context of the negative findings identified several promising future research directions including: novel computer-based support for evidence assessment; formal evaluation of a more comprehensive evidence assessment approach that requires consideration of specific, explicitly stated, clinical consequences; and more formal investigation of DDI case report assessment instruments" /><sentence text="" /></document>